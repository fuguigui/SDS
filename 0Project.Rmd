---
title: "Project: Coronavirus Social Impact Theory"
author: "Guirong Fu"
date: "February 12, 2020"
output: html_document
---

Coronavirus:

- For Twitter hashtags:
#coronavirus #COVID2019 
Any tweet include (one of) these hashtags will be included.

For tweets with other languages: different language of "coronavirus" hashtag will be included.

- For Google Trend search keyword: coronavirus


## Collect Data

```{r rtweet, include=F}
#rm(list=ls())
library(dplyr)
library(rtweet,lib.loc="./lib/")

## store api keys (these are fake example values; replace with your own keys)
api_key <- "VWWcdtFA3fLW92nBryikpgR8B"
api_secret_key <- "77E6tRxJb6wafNe5fJw3iRkelnkEPurkyVqALAb2La9SqAOs07"

## authenticate via web browser
token <- create_token(
  app = "SocialDSFufu",
  consumer_key = api_key,
  consumer_secret = api_secret_key)
#token

#get_token()
```
### Google Trend
Countries: 
Taiwan, Japan, South Korea, Russia, India, Singapore, Italy, Germany, United States, Canada, Australia 

The number is relatively popular level. Update every 30 days. I had better check every week, to see if the data is consistent. 
Finally, I need to normalize and see the trend.

### Twitter
Hashtags (Common) #COVID2019, #coronavirus
add two new hashtags from 2020/3/1 #COVID_19 #corona #covid19
Countries: 
Japan (ja), Russia (ru) ??????????????????????, India (hi), Italy(it), Germany(de), Korean (ko)


languages = c("hi","ja","ko","ru","de","it")


```{r functions}
crabbingGlobal<-function(init_id,init_datetime, end_datetime){
  # input example: init_id: the last wanted  status_id. "1226657019827851265"
  # init_datetime: the earliest created_at time of currently searched tweets'. Format is fixed: %Y-%m-%d %H:%M:%S "2020-02-11 14:00:00"
  # end_datetime: the beginning created_at time of the wanted tweets. The search range of tweets is [end_datetime, init_datetime] 
  
  # initialize
  max_id = init_id
  last_time = init_datetime
  
  while(1){
    if(strptime(last_time, "%Y-%m-%d %H:%M:%S") < strptime(end_datetime, "%Y-%m-%d %H")){
      break
    }
    limit = rate_limit(token = token, "search/tweets")
    n = (limit$remaining-5)*100
  
    en_tweets <- search_tweets(q="#COVID2019 OR #coronavirus OR #COVID_19 OR #corona OR #covid19", lang="en",n=n, max_id=max_id, token=token)
    arrange(en_tweets,created_at)->sorted
    shorted<-select(sorted, user_id, status_id, created_at, is_retweet, hashtags, lang, retweet_status_id, retweet_user_id)
    last_time = shorted$created_at[1]
    dt = strptime(last_time, "%Y-%m-%d %H:%M")
    fname = paste("./data/Tweeten",format(dt,"%m%d%H%M"), ".RData",sep="")
    save(shorted,file=fname)
  
    max_id = sorted$status_id[1]
  
    limit<-rate_limit(token = token, "search/tweets")
    if(limit$remaining<10){
      print("rate limit reached, waiting")
      Sys.sleep(15*60)
    }
  }
  print("Successful crabbing for global!")
}

crabbingData<-function(n, queries, languages,since_ids,names,init_date){
  last_ids = rep(NA,n)
  
  
  for(i in 1:n){
    print(names[i])
    q = queries[i]
    l = languages[i]
    sid= since_ids[i]
    fname = paste("./data/",l,"/",names[i],".RData",sep="")
    date = init_date
    
    lts = rate_limit(token,"search/tweets")
    limit_n = (lts$remaining - 10)*100
    print(limit_n)
    if(limit_n<5){
      print("rate limit reached, waiting")
      Sys.sleep(15*60)
      limit_n = 17000
    }
    
    
    tweets <- search_tweets(q=q, lang = l,n=limit_n, since_id=sid,until=date, token=token)
    arrange(tweets,created_at)->sorted
    tweets<-select(sorted, user_id, status_id, created_at, is_retweet, hashtags, lang, retweet_status_id, retweet_user_id)
    actual_nrow = length(tweets$status_id)
    cid_char = tweets$status_id[1]
    n_cumu=0
    
    # search for more tweets if the current limit cannot search for all of satisfied tweets.
    while(actual_nrow >limit_n-100){
      lts = rate_limit(token,"search/tweets")
      if(lts$remaining <25){
        print("rate limit reached, waiting")
        Sys.sleep(15*60)
      }
      print("Searching for more tweets...")
      n_cumu=n_cumu+actual_nrow
      print(n_cumu)
      
      limit_n=2200
      
      more_tweets<-search_tweets(q=q, lang = l,n=limit_n, max_id=cid_char, token=token)
      arrange(more_tweets,created_at)->sorted
      more_tweets<-select(sorted, user_id, status_id, created_at, is_retweet, hashtags, lang, retweet_status_id, retweet_user_id)
      cid_char = more_tweets$status_id[1]
      actual_nrow = length(more_tweets$status_id)
      
      tweets<-rbind(more_tweets, tweets)
      
      save(tweets,file=fname)
    }
    last_ids[i]<-tweets$status_id[length(tweets$status_id)]
    save(tweets,file=fname)
  }
  return(last_ids)
}

```


Time plot 
```{r}
load("./data/Tweet0209ja.RData")
load("./data/Tweet0209ko.RData")
load("./data/Tweet0209de.RData")
load("./data/Tweet0209hi.RData")
load("./data/Tweet0209ru.RData")
load("./data/Tweet0209it.RData")

load("./data/Tweet0209en1513_short.RData")
allen_tweet<-shorted
load("./data/Tweet0209en2135.RData")
allen_tweet<-rbind(allen_tweet,tweets)
max_id<-allen_tweet$status_id[1]
# allen_tweet$created_at[1]

tweets<-select(en_tweets, user_id, status_id, created_at, is_retweet, hashtags, lang, retweet_status_id, retweet_user_id)

library(dplyr)
#sorted<-arrange(allen_tweet, created_at)
sorted<-arrange(tweets, created_at)
dat<-data.frame(date=sorted$created_at)

dat$hour <- as.POSIXlt(dat$date)$hour
# aggregate(.~hour,data=dat,sum)
hist(dat$hour)



```
 split ids:
 
- hi:#???????????????
  - Feb-09 1226285334578647040 (02-08 23:23:01) 1226295037710884864	(02-09 00:01:35)
  - Feb-10 1226651410281816064 (02-09 23:37:40) >=...
  - Feb-11 1227019372390178818 (02-10 23:59:49) >=...
  - Feb-12 1227379013082943488 (02-11 23:48:54) >=...
  - Feb-13 1227734645166485504 (02-12 )
  - Feb-14 1228105796581380096
  - Feb-15 1228447931113492481
  - Feb-16 1228826047367434241
  - Feb-17 1229188468011487232
  - Feb-18 1229555029654278144
  - Feb-19 1229913233806385152
  - Feb-20 1230280392248184833
  - Feb-21 1230638370864451585
  - Feb-22 1231003399300304897
  - Feb-23 1231364113617453061
  - Feb-24 1231722944319172609
  - Feb-25 1232066843608834049
  - Feb-26 1232436628724994048
  - Feb-27 1232816300700917760
  - Feb-28 1233179448704290817
  - Feb-29 1233541459086757888
  - Mar-01 1233904495769378816
  - Mar-02 1234266680504786947
  
- ko: #?????????
  - Feb-09 1226182331385307138 (02-08 16:33:43) >=...
  - Feb-10 1226651539764170753 (02-09 23:38:11) >=...	
  - Feb-11 1227013464394043392 (02-10 23:36:21) >=...
  - Feb-12 1227379459130396674 (02-11 23:50:41) >=...
  - Feb-13 1227744023135154176
  - Feb-14 1228105509921624064
  - Feb-15 1228467279316209664
  - Feb-16 1228822692561739776
  - Feb-17 1229191221920141312
  - Feb-18 1229553182583353344
  - Feb-19 1229915429226700800
  - Feb-20 1230279697281376257
  - Feb-21 1230641746796666880
  - Feb-22 1231005632716689408
  - Feb-23 1231367458771980289
  - Feb-24 1231730072295358464
  - Feb-25 1232092599529246720
  - Feb-26 1232455178244083712
  - Feb-27 1232817029263941634
  - Feb-28 1233179639863865344
  - Feb-29 1233542368542019584
  - Mar-01 1233903629305729026
  - Mar-02 1234267085288509440
  
- ru: #??????????????????????
  - Feb-09 1226294506783375361 (02-08 23:59:28) 1226295045755670529 (02-09 00:01:37)   
  - Feb-10 1226656614301736960 (02-09 23:58:21) >=1226657075905716224 (02-10 00:00:11)
  - Feb-11 1227019031619756032 (02-10 23:58:28) >=...
  - Feb-12 1227376820447739909 (02-11 23:40:12) >=...
  - Feb-13 1227742201830084609
  - Feb-14 1228106482287120384
  - Feb-15 1228467935716544515
  - Feb-16 1228831181568958465
  - Feb-17 1229192981317595136
  - Feb-18 1229555021882232832
  - Feb-19 1229917113097150465
  - Feb-20 1230277998659526656
  - Feb-21 1230641630740271104
  - Feb-22 1231004674393858048
  - Feb-23 1231366968973889543
  - Feb-24 1231730001613148166
  - Feb-25 1232092053644992512
  - Feb-26 1232454012382801920
  - Feb-27 1232817488775217152
  - Feb-28 1233179116855332864
  - Feb-29 1233541440581623820
  - Mar-01 1233904267355779074
  - Mar-02 1234266802898821122
  
  
- ja: #?????????????????????
  - Feb-09 1226294634915061761 (02-08 23:59:59) 1226294642468999168 (02-09 00:00:00)
  - Feb-10 1226657019827851265 (02-09 23:59:58) 1226657075905716224 (02-10 00:00:11)
  - Feb-11 1227019356388782081 (02-10 23:59:46) 1227019491013316610 (02-11 00:00:18)
  - Feb-12 1227381702873374720 (02-11 23:59:36) 1227381802626535425 (02-12 00:00:00)
  - Feb-13 1227744058530885632 (02-12 23:59:28) 1227744197962321920 (02-13 00:00:01)
  - Feb-14 1228106573324488704
  - Feb-15 1228468932073287680
  - Feb-16 1228831311596421121
  - Feb-17 1229193721444360192
  - Feb-18 1229556061750775808
  - Feb-19 1229918461444284416
  - Feb-20 1230280895229091840
  - Feb-21 1230643232360390658
  - Feb-22 1231005659925139456
  - Feb-23 1231367978370748421
  - Feb-24 1231730453821837312
  - Feb-25 1232092840500461568
  - Feb-26 1232455220992462848
  - Feb-27 1232817604118474752
  - Feb-28 1233180005401645058
  - Feb-29 1233542395540754432
  - Mar-01 1233904779950080000
  - Mar-02 1234267164862844928

- de: (same)
  - Feb-09 1226294623750041600 (02-08 23:59:56) 1226295037710884864	(02-09 00:01:35)
  - Feb-10 1226656904002330625 (02-09 23:59:30) >=1226657075905716224 (02-10 00:00:11)
  - Feb-11 1227019171675938817 (02-10 23:59:02) >=...
  - Feb-12 1227381336891121676 (02-11 23:58:09) >=...
  - Feb-13 1227743720939966464 (02-12 23:58:08) >=...
  - Feb-14 1228106431875796992
  - Feb-15 1228468582419456001
  - Feb-16 1228831307226124289
  - Feb-17 1229193542981029888
  - Feb-18 1229555685622415360
  - Feb-19 1229918220108218368
  - Feb-20 1230280294009376769
  - Feb-21 1230643003842273285
  - Feb-22 1231005557911363584
  - Feb-23 1231368056515047424
  - Feb-24 1231730443684384768
  - Feb-25 1232092800302428161
  - Feb-26 1232455219155390465
  - Feb-27 1232817581989531648
  - Feb-28 1233179939970658305
  - Feb-29 1233542394093719552
  - Mar-01 1233904751059853312
  - Mar-02 1234267165555150848

  
- it: (same)
  - Feb-09 1226294584986218497 (02-08 23:59:47) 1226294643702280192 (02-09 00:00:01)
  - Feb-10 1226656902744023040 (02-09 23:59:30) >=1226657075905716224 (02-10 00:00:11)
  - Feb-11 1227019400286539777 (02-10 23:59:56) 1227019416459628545 (02-11 00:00:00)
  - Feb-12 1227381712646217728 (02-11 23:59:38) >=...
  - Feb-13 1227743994421096450 (02-12 23:59:13) >=...
  - Feb-14 1228106135070269446
  - Feb-15 1228468857385385984
  - Feb-16 1228831345528463361
  - Feb-17 1229193594365513728
  - Feb-18 1229556129128239107
  - Feb-19 1229918051556057089
  - Feb-20 1230280792338837504
  - Feb-21 1230643282885120000
  - Feb-22 1231005675674898432
  - Feb-23 1231368058448547840
  - Feb-24 1231730416375205889
  - Feb-25 1232092843147177984
  - Feb-26 1232455230651912192
  - Feb-27 1232817608002588673
  - Feb-28 1233179982408667138
  - Feb-29 1233542374464401410
  - Mar-01 1233904767883186177
  - Mar-02 1234267169950752768
  


- Feb-24 1231730453821837312
- Feb-25 1232092843147177984
- Feb-26 1232455230651912192
- Feb-27 1232817608002588673
- Feb-28 1233180005401645058
- Feb-29 1233542374464401410


Doing some analysis, keep columns:
user_id, status_id, created_at, is_retweet, hashtags, lang, retweet_status_id, retweet_user_id



```{r democountries}
# setting up the parameters

#q_hi = "#COVID2019 OR #coronavirus OR #COVID_19 OR #corona OR #covid19 OR #???????????????"
#q_ko = "#COVID2019 OR #coronavirus OR #COVID_19 OR #corona OR #covid19 OR #?????????"
#q_ru = "#COVID2019 OR #coronavirus OR #COVID_19 OR #corona OR #covid19 OR #??????????????????????"
#q_en = "#COVID2019 OR #coronavirus OR #COVID_19 OR #corona OR #covid19"
#q_ja = "#COVID2019 OR #coronavirus OR #COVID_19 OR #corona OR #covid19 OR #?????????????????????"


q_hi = "#COVID2019 OR #coronavirus OR #???????????????"
q_ko = "#COVID2019 OR #coronavirus OR #?????????"
q_ru = "#COVID2019 OR #coronavirus OR #??????????????????????"
q_en = "#COVID2019 OR #coronavirus"
q_ja = "#COVID2019 OR #coronavirus OR #?????????????????????"

queries=c(q_hi,q_ko,q_ru,q_ja,q_en,q_en)

languages = c("hi","ko","ru","ja","de","it")

# ids refer to the ealiest ids we want (excluded).
id_hi = "1233179448704290817"
id_ko = "1233179639863865344"
id_ru = "1233179116855332864"
id_ja = "1233180005401645058"
id_de = "1233179939970658305"
id_it = "1233179982408667138"
#ids = c(id_hi,id_ko,id_ru,id_ja,id_de,id_it)

names<-rep(NA,length(languages))
for(i in 1:length(languages)){
  names[i]<-paste("Tweet0229",languages[i],sep="")
}

ids = last_ids
date = "2020-03-01"

#qs = c(q_ru,q_en,q_en)
#ls = c("ru","de","it")
#ids = c(id_ru,id_de,id_it)
#ns<-rep(NA,length(ls))
#for(i in 1:length(ls)){
#  ns[i]<-paste("Tweet0211",ls[i],sep="")
#}
#tws = crabbingData(3,qs,ls,ids,ns,date, init_n = 14000)

# crabbingData<-function(n, queries, languages,since_ids,names,init_date, init_n=17000)
last_ids = crabbingData(6,queries,languages,ids,names,date)

```

```{r demoglobal}
init_id = "1232817608002588673" # the last known id in 2020-02-12
init_datetime = "2020-02-26 23:59:52"
end_datetime = "2020-02-26 12"
crabbingGlobal(init_id, init_datetime, end_datetime)

max_id = "1233180005401645058"
n=10000
ts_exp<-search_tweets(q="#COVID2019 OR #coronavirus", lang="en",n=n, max_id=max_id, token=token)

```

load("./data/ja/Tweet0225ja.RData")
save(tweets,file = "./data/ja/Tweet0225ja.RData")
```{r datarecheck}
Sys.sleep(7*60)

load("./data/it/Tweet0229it.RData")
#rm(shorted)
#sorted$created_at[1]
#sorted$created_at[1416]
#arrange(tweets_ja,created_at)->tweets
#rm(tweets)
# shorted$created_at[20210:20221]
#tweets<-shorted
#tweets<-arrange(sorted,created_at)
tweets$created_at[1:5]
tweets$created_at[nrow(tweets)]

tweets$created_at[2200]
tweets_21<-tweets[2200:nrow(tweets),]
tweets_21$created_at[50:60]

tweets_21<-tweets_21[54:nrow(tweets_21),]
tweets_21$created_at[1]
tweets_21$created_at[nrow(tweets_21)]

arrange(tweets_29,created_at)->tweets
tweets$created_at[2940:2950]

tweets_29$created_at[85:90]
#arrange(tweets_22,created_at)->tweets_21

t<-tweets_27$created_at[nrow(tweets_27)]

t#tweets$created_at[1100:1110]
#tweets<-tweets[1101:nrow(tweets),]

tweets$created_at[nrow(tweets)]

tweets<-rbind(shorted,tweets)

max_id=tweets$status_id[1]

more_tweets<-search_tweets(q=q_ja,lang="ja",n=2500,max_id=max_id,token=token)

arrange(more_tweets,created_at)%>% select(user_id, status_id, created_at, is_retweet, hashtags, lang, retweet_status_id, retweet_user_id)->shorted
shorted$created_at[1]
shorted$created_at[1005:1010]
shorted<-shorted[1010:nrow(shorted),]

rate_limit(token,"search/tweets")

save(tweets_21,file = "./data/it/Tweet0221it.RData")
```


```{r endatacheck}
check_tweets<-NULL
load("./data/Tweeten02262244.RData")
tweets<-arrange(shorted,created_at)
tweets$created_at[1]
tweets$created_at[nrow(tweets)]
tweets$created_at[11340]
tweets$created_at[11340:11350]
tweets<-tweets[11343:nrow(tweets),]

check_tweets<-rbind(check_tweets,tweets)

check_tweets$created_at[1]
check_tweets$created_at[nrow(check_tweets)]
check_tweets<-check_tweets[1:(nrow(check_tweets)-5),]

save(check_tweets,file = "./data/Tweet0226en.RData")
```