---
title: "2DataView"
author: "Guirong Fu"
date: "March 24, 2020"
output: html_document
---

Main task: have an overview of the data

- Impact: 
  
    - Google trend
    - Twitter data pattern

- Infection statistics

- Immediacy measure (geo-data?)

```{r prepare, message=FALSE}
rm(list=ls())
library(dplyr)
library(ggplot2)
library(reshape2)
```

```{r googleimpact}
getwd() %>% paste0("/data/")->path
files = dir(path)
googlefile = files[grep("Google\\s?",files)]
tables<-c()
for(f in googlefile){
  gtable<-read.csv(paste0(path,f),stringsAsFactors = FALSE)
  colnames(gtable)<-gtable[2,]
  gtable$Day<-as.Date(gtable$Day,"%m/%d/%Y")
  tab<-gtable[3:nrow(gtable),]
  tables<-rbind(tables,tab)
}
View(tables)
plot(tables$`United States`[13:32],type="l")
lines(tables$`United States`[33:(32-13+33)],type="l",col="red",add=T)
dat<-tables[,2:ncol(tables)]
dat_num<-sapply(dat,as.integer)
dat_num
diff_first<-dat_num[33:(32-13+33),]-dat_num[13:32,]
plot(diff_first,type="l")

# Question: how to regularize Google trend index???
apply(diff_first,MARGIN=2,mean)

PlotGooglgeDiff<-function(cname,dat){
  
  plot_list<-list(x1=dat[33:(32-13+33),cname],x2=dat[13:32,cname],title=cname)
  plot(plot_list$x1,type="l",main=plot_list$title,ylim = c(min(plot_list$x1,plot_list$x2)-2,max(plot_list$x1,plot_list$x2)+2))
  lines(plot_list$x2,col="red")

  plot(plot_list$x1,plot_list$x2,xlab = "Later Records",ylab="Earlier Records",main=plot_list$title)
}
PlotGooglgeDiff("Canada",dat_num)
PlotGooglgeDiff("United States",dat_num)
PlotGooglgeDiff("Taiwan",dat_num)

# Solution: the overlapped part take the average, the outer parts are rectified by the bias.
# bias is defined by the average of the difference between the later and earlier records.
# the rectified earlier record = earlier record + bias/2,
# the rectified later record = later record - bias/2
# for example, all the later records are 5 larger than the earlier ones.
# earlier ones: 1,2,3,(4,5,6,5,4,3,2,2)(overlapped ones)
# later ones:   (9,10,11,10,9,8,7,6,6),8,9,10,10,10
# bias: 5, bias/2: 2.5
# rectified earlier: 3.5,4.5,5.5,(6.5,7.5,8.5,7.5,6.5,5.5,4.5,4.5)
# recfified later:  (6.5,7.5,8.5,7.5,6.5,5.5,4.5,4.5),5.5,6.5,7.5,7.5,7.5

RectifyBias<-function(cname,dat,start,step,end){
  table<-data.frame(later=dat[(start+step+1):(start+step*2+1),cname],earlier = dat[start:(start+step),cname])
  bias<-mean(table$later-table$earlier)
  overlapped<-apply(table,MARGIN = 1,mean)
  table_rec<-c(dat[1:(start-1),cname]+bias/2,overlapped,dat[(start+step*2+2):end,cname]-bias/2)
  return(table_rec)
}
start=13
step=32-13
end=50
rec_canada<-RectifyBias("Canada",dat_num,start,step,end)

plot(rec_canada[1:(start+step)],type="l",main="Compare Rectificaiton (Earlier)")
lines(dat_num[1:(start+step),"Canada"],col="red")

plot(rec_canada[start:length(rec_canada)],type="l",main="Compare Rectificaiton (Later)")
lines(dat_num[(start+step+1):(length(rec_canada)+step+1),"Canada"],col="blue")

# overlapped:
# the first part: 11-30,31-50,
countries<-colnames(dat_num)
countries
overlap_first<-sapply(countries, RectifyBias, dat=dat_num, start=11,step=30-11,end=60)
overlap_first

date_first<-c(tables$Day[1:10],tables$Day[31:60])
date_first

plot(overlap_first[,2],type="l")
lines(dat_num[31:60,2],col="red")

# the second part:
dat_num_sec<-rbind(overlap_first,dat_num[61:nrow(dat_num),])
date_second<-c(date_first,tables$Day[61:nrow(tables)])
date_second

# 23-40,41-58
overlap_second<-sapply(countries,RectifyBias,dat=dat_num_sec,start=23,step=40-23,end=nrow(dat_num_sec))
date_final<-c(date_second[1:22],date_second[41:length(date_second)])

cid=5
plot(date_final,overlap_second[,cid],type="l",main=countries[cid])

Standarize100<-function(cname,dat,minv=1,maxv=100){
  vec=dat[,cname]
  maxd=max(vec)
  mind=min(vec)
  k = (maxv-minv)/(maxd-mind)
  c = (maxd*minv-mind*maxv)/(maxd-mind)
  return(k*vec+c)
  
}

#dat_stand<-Standarize100(countries[cid],overlap_second)
#plot(date_final,overlap_second[,cid],type="l",main=countries[cid])
#lines(date_final,dat_stand,col="red")

dat_stand<-sapply(countries,Standarize100,dat=overlap_second)
df_plot<-melt(dat_stand)
df_plot["Day"]<-rep(date_final,ncol(dat_stand))
ggplot(df_plot, aes(x=Day, y=value)) + geom_line(aes(color=Var2))

#write.csv(cbind(Day=as.character(date_final),dat_stand),file=paste0(path,"google_rectified.csv"))
```


```{r strength}
strength<-read.csv(paste0(path,"worldwide.csv"),header = F, stringsAsFactors = F)[1:108,1:42]
strength<-t(strength)
strength[1,1]="Date"
strength<-as.matrix(strength)
colnames(strength)<-strength[1,]
dat_strength<-as.data.frame(strength[2:nrow(strength),],stringsAsFactors = F)
dates_strength<-as.Date(dat_strength$Date,"%m/%d/%Y")
dat_no_date<-dat_strength[,2:ncol(dat_strength)]
dat_no_date[dat_no_date==""]="0"
dat_strength_num<-sapply(dat_no_date, as.integer)


cname="Germany"

ex_impact<-dat_stand[9:nrow(dat_stand),cname]
ex_diff<-c(0,diff(dat_strength_num[,cname]))
cor.test(ex_impact,ex_diff)

plot(ex_impact,type="l",ylim=c(1,100))
lines(dat_strength_num[,cname]/max(dat_strength_num[,cname])*100,col="red")
lines(dat_strength_num[,"overall"]/max(dat_strength_num[,"overall"])*100,col="blue")
lines(ex_diff/max(ex_diff)*100,col="yellow")

date_final
View(strength)
sum(strength=="")
#strength[106,]

```


```{r twitter-view-en}

twitterfiles = files[grep("Tweet\\s?",files)]

# use each hour as unit, check how many retweet, tweet
ExtractHours<-function(tm){
  t<-as.POSIXlt(tm)
  t$hour
}

for(i in 1:3){
  fname<-twitterfiles[i]
  load(paste0(path,fname))
  twit_demo<-check_tweets
  hours<-sapply(twit_demo$created_at,ExtractHours)
  data.frame(hour=hours,retweet=twit_demo$is_retweet) %>% group_by(hour)%>%summarise(cnt=sum(retweet),ttl=n())->twit_stat

  twit_plot<-melt(twit_stat,id="hour")
  ggplot(twit_plot, aes(fill=variable, y=value, x=hour)) + 
    geom_bar(position = 'dodge', stat="identity")+labs(title=fname)

}


```


```{r twitter-de-view}
defiles = dir(paste0(path,"de/"))

fname<-defiles[1]
load(paste0(path,"de/",fname))
twit_demo<-tweets
hours<-sapply(twit_demo$created_at,ExtractHours)
data.frame(hour=hours,retweet=twit_demo$is_retweet) %>% group_by(hour)%>%summarise(cnt=sum(retweet),ttl=n())->twit_stat

twit_plot<-melt(twit_stat,id="hour")
ggplot(twit_plot, aes(fill=variable, y=value, x=hour)) + 
  geom_bar(position = 'dodge', stat="identity")+labs(title=fname)


```


```{r twitter-ru-view}
rufiles = dir(paste0(path,"ru/"))

fname<-rufiles[1]
load(paste0(path,"ru/",fname))
twit_demo<-tweets
hours<-sapply(twit_demo$created_at,ExtractHours)
data.frame(hour=hours,retweet=twit_demo$is_retweet) %>% group_by(hour)%>%summarise(cnt=sum(retweet),ttl=n())->twit_stat

twit_plot<-melt(twit_stat,id="hour")
ggplot(twit_plot, aes(fill=variable, y=value, x=hour)) + 
  geom_bar(position = 'dodge', stat="identity")+labs(title=fname)


```


```{r twitter-ko-view}
kofiles = dir(paste0(path,"ko/"))
fname<-kofiles[20]

load(paste0(path,"ko/",fname))
twit_demo<-tweets
hours<-sapply(twit_demo$created_at,ExtractHours)
data.frame(hour=hours,retweet=twit_demo$is_retweet) %>% group_by(hour)%>%summarise(cnt=sum(retweet),ttl=n())->twit_stat

twit_plot<-melt(twit_stat,id="hour")
ggplot(twit_plot, aes(fill=variable, y=value, x=hour)) + 
  geom_bar(position = 'dodge', stat="identity")+labs(title=fname)


```



```{r twitter-hi-view}
hifiles = dir(paste0(path,"hi/"))
fname<-hifiles[20]

load(paste0(path,"hi/",fname))
twit_demo<-tweets
hours<-sapply(twit_demo$created_at,ExtractHours)
data.frame(hour=hours,retweet=twit_demo$is_retweet) %>% group_by(hour)%>%summarise(cnt=sum(retweet),ttl=n())->twit_stat

twit_plot<-melt(twit_stat,id="hour")
ggplot(twit_plot, aes(fill=variable, y=value, x=hour)) + 
  geom_bar(position = 'dodge', stat="identity")+labs(title=fname)


```



```{r twitter-it-view}
itfiles = dir(paste0(path,"it/"))
fname<-itfiles[20]

load(paste0(path,"it/",fname))
twit_demo<-tweets
hours<-sapply(twit_demo$created_at,ExtractHours)
data.frame(hour=hours,retweet=twit_demo$is_retweet) %>% group_by(hour)%>%summarise(cnt=sum(retweet),ttl=n())->twit_stat

twit_plot<-melt(twit_stat,id="hour")
ggplot(twit_plot, aes(fill=variable, y=value, x=hour)) + 
  geom_bar(position = 'dodge', stat="identity")+labs(title=fname)


```

# Strength

create features to measure strength for each target: from the following data:

- accumulated confirmed case number

- accumulated recovered case number

- accumulated death case number

- total population.

The features are:

Absolute index

- confirmed/recovered/death case number: accumulated value, daily newly increasing number

- the currently existing case: accumulated confirmed value minus accumulated recovered or dead value

Relative index:

- nation size effect: accumulated value /the total population

- the daily newly recovered rate: today's newly covered number/the newly recovered number on one day ago

- the death rate: accumulated death number/accumulated confirmed number

- the heal rate: accumulated recovered number/accumulated confirmed number

- the "risk" level: 

  - daily recoverd number/daily death number
  
  - accumulated recovered numbr/accumulated death number

```{r strengthFeature}
ExtractStrengthFeature<-function(dat, pop){
  # the data is n*3 matrix(data frame)
  # row: each row represents one day. Ordered from the old to new, without missing days
  # column: accumulated confirmed, accumulated recovered, accumulated dead
  
  # daily newly absolute values
  dat_daily<-apply(dat,MARGIN = 2,diff)
  dat_daily<-as.data.frame(rbind(c(0,0,0),dat_daily))
  rownames(dat_daily)<-rownames(dat)
  
  # ratio to the population
  dat_ratio_pop<-dat/pop*10000
  dat_exist<-dat$confirm-dat$recover-dat$death
  
  # relative index
  # the daily newly recovered rate
  recover_lagged<-c(1,dat_daily$recover[2:nrow(dat_daily)])
  recover_lagged[recover_lagged==0]<-1
  dat_daily_recover_rat<-dat_daily$recover/recover_lagged
  
  # the death rate
  confirm_nonzero<-dat$confirm
  confirm_nonzero[confirm_nonzero==0]<-1
  dat_death_rate<-dat$death/confirm_nonzero
  
  
  # the heal rate
  dat_heal_rate<-dat$recover/confirm_nonzero
  
  # daily recoverd number/daily death number
  death_nonzero<-dat_daily$death
  death_nonzero[death_nonzero==0]<-1
  dat_risk_daily<-dat_daily$recover/death_nonzero
  
  # accumulated recovered numbr/accumulated death number
  death_nonzero<-dat$death
  death_nonzero[death_nonzero==0]<-1
  dat_risk_ttl<-dat$recover/death_nonzero
  
  dat_features<-cbind(dat,dat_daily,dat_ratio_pop,dat_exist,dat_daily_recover_rat,dat_death_rate,dat_heal_rate,dat_risk_daily,dat_risk_ttl)
  head(dat_features)
  oldnames<-colnames(dat)
  colnames(dat_features)<-c(oldnames,sapply(oldnames,paste0,"_daily"),sapply(oldnames,paste0,"_RP"),"Exists","ReDR","DeathR","HealR","RiskD","RiskT")
  # _RP: ratio to the total population
  # Exists: the accumulated existing case
  # ReDR: recover daily ratio: today's recover/yesterday's recover
  # DeathR: accumulated death/accumulated confirmed
  # HealR: accumulated recover/ accumulated confirmed
  # RiskD: daily risk: daily recoverd/daily death
  # RiskT: total risk: accumualted recoverd/accumulated death
  return(dat_features)
}

```


```{r}
getwd() %>% paste0("/data/")->path
files=dir(path)
covid = files[grep("[0-9]?covid\\s?",files)]
ReadCovid<-function(fname){
  covid1<-as.data.frame(t(read.csv(paste0(path,fname),header = F, stringsAsFactors = F)),stringsAsFactors = F)
  rownames(covid1)<-covid1[,1]
  colnames(covid1)<-covid1[2,]
  covid1<-covid1[5:nrow(covid1),2:ncol(covid1)]
  return(covid1)
}

covid_confirm<-ReadCovid(covid[1])
covid_deadnum<-ReadCovid(covid[2])
covid_recover<-ReadCovid(covid[3])

covid_demo<-data.frame(confirm=as.numeric(covid_confirm$Italy)[1:63],
                       recover=as.numeric(covid_recover$Italy),
                       death=as.numeric(covid_deadnum$Italy)[1:63])
rownames(covid_demo)<-rownames(covid_recover)
head(covid_demo)
```

```{r get-population, message=FALSE}
library(rjson)
library(jsonlite)

#pop_json<-jsonlite::stream_in(file(paste0(path,"world_population.json")),pagesize = 500)
pop_dat<-fromJSON(paste0(readLines(paste0(path,"world_population.json"))))
pop_data<-as.integer(pop_dat$population)
names(pop_data)<-pop_dat$country

pop_demo=pop_data["Italy"]
```

```{r extract-features-demo}
features_demo<-ExtractStrengthFeature(covid_demo,pop_demo)
head(features_demo)
features_demo[30:40,]
```

# Immediacy

From the social network view, I use two ways to measure immediacy

- the length of the shortest path

- the high power of transition matrix.

Each target/source is viewed as a node in a network. For the two have border touch, there exists an edge between these two nodes. In this way, the two targets(or sources) who are contiguous to each other, are connected in the network.

## Shortest path

The length of the shortest path is the classic measurment of the distance between a pair of nodes in a graph.

## High power of transition matrix

This is a self-created immediacy measurement, inspired by the eigenvector centrality in social networks. 

The element at (i,j) in the high power t of the transition matrix could be viewed as the probability of going from i to j in t steps. 


- If the matrix is aperiodic, the elements would converge to constant values. We use large finite power times to approximate the final constat values.

- If the matrix is periodic, there would be several converged matrixs with big enough power times. We simulate by multiple times of high finite power and take the average of them. 

Here, we assume we are given an undirected network without isolate nodes, which means the matrix is irreducible.

We create this measurement to distinguish the following case which is the same in the shortest-path measurement:

Two nodes have the same length of shortest path from an anchored node, but different numbers of shortest paths.

Intuitively, the node with more shortest paths should be more immediate to the anchored node.

One issue of this measurement is this is unsymmetric, applicable for directed network, instead of undirected network. Maybe only for an additional choice???

A toy example:
A-B
A-C
A-D
A-E
D-F
E-F

The length of the shortest path between (A,C) is 2, the same for (A,F).
However, there is only one path (A-B-C) with length 2 between A and C, but 2 (A-D-F, A-E-F)with respect for A and F.
We will say F is more immediate to A than C.

```{r immediacy}
mat<-matrix(c(0,1,0,1,1,0,
                1,0,1,0,0,0,
                0,1,0,0,0,0,
                1,0,0,0,0,1,
                1,0,0,0,0,1,
                0,0,0,1,1,0),nrow=6)

g<-graph.adjacency(mat,mode = "undirected")
shortest.paths(g)
diag_mat<-diag(1/degree(g))
mat_weight = t(mat)%*%diag_mat

ImmediacyProb<-function(mat, iter=20,powertimes=7){
  mat_list<-list()
  mat_sum<-matrix(0,ncol = ncol(mat),nrow=nrow(mat))
  for(i in 1:iter){
    base_mat = mat
    pot<-1
    for(j in 1:powertimes){
      base_mat = base_mat %*% base_mat
      pot<-2*pot
      rand_number<-runif(1)
      if(rand_number<0.5){
        base_mat = base_mat %*% mat
        pot<-pot+1
      }
    }
    print(pot)
    mat_list[[i]]<-base_mat
    mat_sum = mat_sum + base_mat
  }
  mat_sum/iter
}

im_prob = ImmediacyProb(mat_weight,powertimes = 8,iter=1)
im_prob

# [3,1] (A->C) has lower value than [6,1] (A->F) in the second measurement but the same in the shortest path measurement.

```


