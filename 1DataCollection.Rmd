---
title: "1DataCollection"
author: "Guirong Fu"
date: "March 3, 2020"
output: html_document
---

## Initialization

```{r init, include=F}
#rm(list=ls())
library(dplyr)
library(rtweet,lib.loc="./lib/") # you can change to your lib location

api_key <- "VWWcdtFA3fLW92nBryikpgR8B"
api_secret_key <- "77E6tRxJb6wafNe5fJw3iRkelnkEPurkyVqALAb2La9SqAOs07"

access_token <- "790192685256613888-lxUV5nhjYhijjBpFDO3gBcTAaSVxT2Q"
access_token_secret <- "EXuLZZjl8mqX7VBQP4yK3QbXXAGDUZID7SMbPgdBjA15F"

## authenticate via web browser
token <- create_token(
  app = "SocialDSFufu",
  consumer_key = api_key,
  consumer_secret = api_secret_key,
  access_token = access_token,
  access_secret = access_token_secret)
```

## Functions


```{r functions}
crabbingGlobal<-function(q, init_id, fdate, end_datetime){
  max_id = init_id
  fname<-paste("./data/Tweet",fdate,"en.RData",sep="")
  n = 17500
  end_date_num<-as.numeric(as.POSIXlt.character(end_datetime,tz="GMT",format="%Y-%m-%d %H"))
  
  en_tweets <- search_tweets(q=q, lang="en",n=n, max_id=max_id, token=token)
  arrange(en_tweets,created_at)%>% select(user_id, status_id, created_at, is_retweet, hashtags, lang, retweet_status_id, retweet_user_id, location)->tweets
  save(tweets,file=fname)
    
  last_time = tweets$created_at[1]
  max_id = tweets$status_id[1]
  print(paste("Backing search to",last_time))
  n = 50
  
  while(as.numeric(last_time) > end_date_num){
    #limit = rate_limit(token = token, "search/tweets")
    #if(limit$remaining<10){
    if(n<100){
      print("rate limit reached, waiting")
      Sys.sleep(15*60)
      n = 17500
    }
    #limit = rate_limit(token = token, "search/tweets")
    # n = (limit$remaining-5)*100
    en_tweets <- search_tweets(q=q, lang="en",n=n, max_id=max_id, token=token)
    arrange(en_tweets,created_at)%>% select(user_id, status_id, created_at, is_retweet, hashtags, lang, retweet_status_id, retweet_user_id, location)->shorted
    
    last_time = shorted$created_at[1]
    max_id = shorted$status_id[1]
    print(paste("Backing search to",last_time))
    
    tweets<-rbind(shorted, tweets)
    save(tweets,file=fname)
    n = 50
  }
  
  keep_sign<-as.numeric(tweets$created_at) > end_date_num
  tweets<-tweets[keep_sign,]
  save(tweets,file=fname)
  
  print("Successful crabbing for global!")
}

crabbingData<-function(n, queries, languages,since_ids,init_date, end_date){
  last_ids = rep(NA,n)
  init_date_num<-as.numeric(as.POSIXlt.character(init_date,tz="GMT",format="%m%d"))
  limit_n=17100
  n_cumu = 0
  
  for(i in 1:n){
    q = queries[i]
    l = languages[i]
    sid= since_ids[i]
    
    print(paste("Searching for",l))
    fname = paste("./data/",l,"/Tweet",init_date,l,".RData",sep="")
    
    #lts = rate_limit(token,"search/tweets")
    #limit_n = (lts$remaining - 10)*100
    print(paste("Left searching times: ",limit_n))
    if(limit_n<500){
      print("rate limit reached, waiting")
      Sys.sleep(15*60)
      limit_n = 17100
    }
    
    
    tweets <- search_tweets(q=q, lang = l,n=limit_n, since_id=sid,until=end_date, token=token)
    arrange(tweets,created_at)%>% select(user_id, status_id, created_at, is_retweet, hashtags, lang, retweet_status_id, retweet_user_id,location)->tweets
    
    actual_time = tweets$created_at[1]
    cid_char = tweets$status_id[1]
    n_cumu=nrow(tweets)
    limit_n = limit_n - n_cumu-100
    
    # search for more tweets if the current earlist time is less than the predefined time
    while(as.numeric(actual_time)>init_date_num){
      
      #lts = rate_limit(token,"search/tweets")
      #limit_n = lts$remaining
      if(limit_n <2500){
        print("rate limit reached, waiting")
        Sys.sleep(15*60)
        limit_n=17100
        print("Searching for more tweets...")
      }
      
      unit_n=2000
      
      more_tweets<-search_tweets(q=q, lang = l,n=unit_n, max_id=cid_char, token=token)
      limit_n = limit_n - unit_n
      arrange(more_tweets,created_at) %>% select(user_id, status_id, created_at, is_retweet, hashtags, lang, retweet_status_id, retweet_user_id, location)->more_tweets
      
      cid_char = more_tweets$status_id[1]
      actual_time = more_tweets$created_at[1]
      
      tweets<-rbind(more_tweets, tweets)
      save(tweets,file=fname)
      
      n_cumu<-nrow(tweets)
      print(paste("Cumulative search numbers:",n_cumu))
    }
    last_ids[i]<-tweets$status_id[length(tweets$status_id)]
    keep_sign<-as.numeric(tweets$created_at)>init_date_num
    tweets<-tweets[keep_sign,]
    save(tweets,file=fname)
  }
  return(last_ids)
}
```

## Usage demo

- hi:#कोरोन
  - Feb-26 1232436628724994048
  - Feb-27 1232816300700917760
  - Feb-28 1233179448704290817
  - Feb-29 1233541459086757888
  - Mar-01 1233904495769378816 (2020-02-29 23:59)
  - Mar-02 1234266680504786947
  - Mar-03 1234626122836475904
  - Mar-04 1234991455065608192
  
- ko: #코로나
  - Feb-26 1232455178244083712
  - Feb-27 1232817029263941634
  - Feb-28 1233179639863865344
  - Feb-29 1233542368542019584
  - Mar-01 1233903629305729026 (2020-02-29 23:59)
  - Mar-02 1234267085288509440 (2020-03-01 23:59)
  - Mar-03 1234629518536495104
  - Mar-04 1234991623794126850
  
- ru: #коронавирус
  - Feb-26 1232454012382801920
  - Feb-27 1232817488775217152
  - Feb-28 1233179116855332864
  - Feb-29 1233541440581623820
  - Mar-01 1233904267355779074 (2020-02-29 23:59)
  - Mar-02 1234266802898821122
  - Mar-03 1234629386395115533
  - Mar-04 1234990709502922753
  
  
- ja: #コロナウイルス
  - Feb-27 1232817604118474752
  - Feb-28 1233180005401645058
  - Feb-29 1233542395540754432
  - Mar-01 1233904779950080000 (2020-02-29 23:59)
  - Mar-02 1234267164862844928
  - Mar-03 1234629551545704448
  - Mar-04 1234991944473858050

- de: (same)
  - Feb-27 1232817581989531648
  - Feb-28 1233179939970658305
  - Feb-29 1233542394093719552
  - Mar-01 1233904751059853312 (2020-02-29 23:59)
  - Mar-02 1234267165555150848
  - Mar-03 1234629554129506306
  - Mar-04 1234991899087273984

  
- it: (same)
  - Feb-27 1232817608002588673
  - Feb-28 1233179982408667138
  - Feb-29 1233542374464401410
  - Mar-01 1233904767883186177 (2020-02-29 23:59)
  - Mar-02 1234267169950752768
  - Mar-03 1234629522613403649
  - Mar-04 1234991932373258241



```{r demo-languages}
# setting up the parameters
q_en = "#COVID2019 OR #coronavirus"
q_hi = paste(q_en, "OR #कोरो")
q_ko = paste(q_en, "OR #코로나")
q_ru = paste(q_en, "OR #коронавирус")
q_ja = paste(q_en, "OR #コロナウイルス")

queries=c(q_hi,q_ko,q_ru,q_ja,q_en,q_en)

languages = c("hi","ko","ru","ja","de","it")

# ids refer to the ealiest ids we want (excluded).
id_hi = "1234991455065608192"
id_ko = "1234991623794126850"
id_ru = "1234990709502922753"
id_ja = "1234991944473858050"
id_de = "1234991899087273984"
id_it = "1234991932373258241"
ids = c(id_hi,id_ko,id_ru,id_ja,id_de,id_it)

# ids = last_ids
init_date = "0304"
end_date = "2020-03-05"
last_ids = crabbingData(6,queries,languages,ids,init_date,end_date)


# if you need to customize the query of language, please refer to the following code. This is an example for 3 languages.
qs= c(q_ko, q_ru, q_ja,q_en,q_en)
ls = c("ko","ru","ja","de","it")
#is = c(id_ja,id_de,id_it)
#ids = last_ids[2:6]
ids = c(id_ko,id_ru,id_ja,id_de,id_it)
last_ids = crabbingData(5,qs,ls,ids,init_date,end_date)

tweets <- search_tweets(q=q_ko, lang = "ko",n=500, since_id=id_ko,until=end_date, token=token)

arrange(tweets,created_at)%>% select(user_id, status_id, created_at, is_retweet, hashtags, lang, retweet_status_id, retweet_user_id,location)->tweets
    
actual_time = tweets$created_at[1]

```


en:
- Feb-27 1232817608002588673
- Feb-28 1233180005401645058
- Feb-29 1233542374464401410 (2020-02-28 23:59)
- Mar-01 1233904779950080000
- Mar-02 1234267169950752768
- Mar-03 1234629554129506306
- Mar-04 1234991944473858050




```{r demoen}
init_id = "1234629554129506306" # the last known id in 2020-02-12
end_datetime = "2020-03-02 12"
q = "#COVID2019 OR #coronavirus"
fdate="0302"

crabbingGlobal(q,init_id,fdate, end_datetime)
```